{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Label\n",
      "0    ACL.N0000\n",
      "1   ASIR.N0000\n",
      "2    BFL.N0000\n",
      "3   BOGA.N0000\n",
      "4   BRWN.N0000\n",
      "5    CCS.N0000\n",
      "6   CFIN.N0000\n",
      "7   CFLB.N0000\n",
      "8   CFVF.N0000\n",
      "9    CIC.N0000\n",
      "10   CIC.X0000\n",
      "11  CIND.N0000\n",
      "12  CINV.N0000\n",
      "13  COMB.N0000\n",
      "14   CTC.N0000\n",
      "15  DFCC.N0000\n",
      "16  DIPD.N0000\n",
      "17  DIST.N0000\n",
      "18  EAST.N0000\n",
      "19  GHLL.N0000\n",
      "20  GLAS.N0000\n",
      "21  GRAN.N0000\n",
      "22  HAYC.N0000\n",
      "23  HAYL.N0000\n",
      "24   HNB.N0000\n",
      "25   JKH.N0000\n",
      "26   LMF.N0000\n",
      "27   LWL.N0000\n",
      "28   NDB.N0000\n",
      "29  NEST.N0000\n",
      "30  ONAL.N0000\n",
      "31  OSEA.N0000\n",
      "32  PALM.N0000\n",
      "33  PARQ.N0000\n",
      "34   PDL.N0000\n",
      "35   RCL.N0000\n",
      "36  RICH.N0000\n",
      "37  SAMP.N0000\n",
      "38  SEMB.N0000\n",
      "39  SEYB.N0000\n",
      "40  SHOT.N0000\n",
      "41  SPEN.N0000\n",
      "42  TILE.N0000\n",
      "43  TKYO.N0000\n",
      "44  TYRE.N0000\n",
      "45  CARG.N0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./all_data.csv')\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "grouped = data[['Label']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Display the result\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "X (scaled): [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "y (scaled): [[0.01972244]\n",
      " [0.01557409]\n",
      " [0.01472378]\n",
      " [0.01272304]\n",
      " [0.00935037]\n",
      " [0.01033359]\n",
      " [0.00935609]\n",
      " [0.00765451]\n",
      " [0.00620636]\n",
      " [0.00211255]\n",
      " [0.00588692]\n",
      " [0.00584432]\n",
      " [0.00543057]\n",
      " [0.00496509]\n",
      " [0.0019258 ]\n",
      " [0.00117075]\n",
      " [0.00131885]\n",
      " [0.        ]\n",
      " [0.00218135]\n",
      " [0.00512025]\n",
      " [0.00418929]\n",
      " [0.01399804]\n",
      " [0.02973882]\n",
      " [0.02741415]\n",
      " [0.03368503]\n",
      " [0.03444531]\n",
      " [0.03640031]\n",
      " [0.03884692]\n",
      " [0.04010453]\n",
      " [0.03900698]\n",
      " [0.05863601]\n",
      " [0.0555159 ]\n",
      " [0.04349625]\n",
      " [0.03256929]\n",
      " [0.02901474]\n",
      " [0.02651668]\n",
      " [0.02764281]\n",
      " [0.02901474]\n",
      " [0.03347352]\n",
      " [0.04399167]\n",
      " [0.05556418]\n",
      " [0.04193377]\n",
      " [0.04345366]\n",
      " [0.05301784]\n",
      " [0.05890871]\n",
      " [0.05777088]\n",
      " [0.0542367 ]\n",
      " [0.05218516]\n",
      " [0.05218516]\n",
      " [0.05150367]\n",
      " [0.04830922]\n",
      " [0.04856478]\n",
      " [0.04856478]\n",
      " [0.0459976 ]\n",
      " [0.04711663]\n",
      " [0.04639255]\n",
      " [0.04639255]\n",
      " [0.04175847]\n",
      " [0.04458236]\n",
      " [0.05290923]\n",
      " [0.04687527]\n",
      " [0.05005818]\n",
      " [0.06232221]\n",
      " [0.0650737 ]\n",
      " [0.04422033]\n",
      " [0.04335144]\n",
      " [0.05060124]\n",
      " [0.05547641]\n",
      " [0.05182312]\n",
      " [0.0478407 ]\n",
      " [0.04639255]\n",
      " [0.03987587]\n",
      " [0.04277217]\n",
      " [0.04476338]\n",
      " [0.03851823]\n",
      " [0.0382467 ]\n",
      " [0.03596587]\n",
      " [0.03118697]\n",
      " [0.02792863]\n",
      " [0.02684251]\n",
      " [0.02086889]\n",
      " [0.03335919]\n",
      " [0.03553142]\n",
      " [0.04422033]\n",
      " [0.03028187]\n",
      " [0.04530644]\n",
      " [0.04253082]\n",
      " [0.04542712]\n",
      " [0.0449444 ]\n",
      " [0.04458236]\n",
      " [0.03933282]\n",
      " [0.03797517]\n",
      " [0.04123351]\n",
      " [0.04096199]\n",
      " [0.03533394]\n",
      " [0.02792863]\n",
      " [0.02881726]\n",
      " [0.02708387]\n",
      " [0.03056633]\n",
      " [0.02814585]\n",
      " [0.02684251]\n",
      " [0.02952585]\n",
      " [0.02684251]\n",
      " [0.02427534]\n",
      " [0.0257564 ]\n",
      " [0.03495978]\n",
      " [0.02544608]\n",
      " [0.02498061]\n",
      " [0.0238014 ]\n",
      " [0.02467029]\n",
      " [0.02622188]\n",
      " [0.02973882]\n",
      " [0.04168606]\n",
      " [0.05257504]\n",
      " [0.0746315 ]\n",
      " [0.09309542]\n",
      " [0.08812217]\n",
      " [0.08459818]\n",
      " [0.08366339]\n",
      " [0.08896819]\n",
      " [0.0806827 ]\n",
      " [0.11855874]\n",
      " [0.14963048]\n",
      " [0.45923627]\n",
      " [0.50267445]\n",
      " [0.40003103]\n",
      " [0.40100853]\n",
      " [0.36250323]\n",
      " [0.32971296]\n",
      " [0.36150116]\n",
      " [0.31012641]\n",
      " [0.26544445]\n",
      " [0.24165158]\n",
      " [0.26195409]\n",
      " [0.25324776]\n",
      " [0.24230023]\n",
      " [0.2118651 ]\n",
      " [0.18986294]\n",
      " [0.17866972]\n",
      " [0.16681536]\n",
      " [0.19812258]\n",
      " [0.19970601]\n",
      " [0.18507207]\n",
      " [0.16688724]\n",
      " [0.18251875]\n",
      " [0.18234838]\n",
      " [0.16358935]\n",
      " [0.15579378]\n",
      " [0.13050599]\n",
      " [0.11948159]\n",
      " [0.11848332]\n",
      " [0.12228472]\n",
      " [0.13478546]\n",
      " [0.14072925]\n",
      " [0.1611068 ]\n",
      " [0.15729678]\n",
      " [0.15283165]\n",
      " [0.15730388]\n",
      " [0.18360486]\n",
      " [0.19741946]\n",
      " [0.18483579]\n",
      " [0.16778168]\n",
      " [0.16532196]\n",
      " [0.18093483]\n",
      " [0.19639051]\n",
      " [0.1484872 ]\n",
      " [0.11445565]\n",
      " [0.07667595]\n",
      " [0.10458536]\n",
      " [0.10426687]\n",
      " [0.08732187]\n",
      " [0.09775932]\n",
      " [0.12214895]\n",
      " [0.15696923]\n",
      " [0.16234807]\n",
      " [0.14860153]\n",
      " [0.1538606 ]\n",
      " [0.15448668]\n",
      " [0.13702267]\n",
      " [0.14115593]\n",
      " [0.14350386]\n",
      " [0.15308721]\n",
      " [0.14912609]\n",
      " [0.16111327]\n",
      " [0.1711749 ]\n",
      " [0.18465477]\n",
      " [0.20664363]\n",
      " [0.19201862]\n",
      " [0.29202793]\n",
      " [0.47175795]\n",
      " [0.34819485]\n",
      " [0.50042739]\n",
      " [0.49604344]\n",
      " [0.46716561]\n",
      " [0.44584526]\n",
      " [0.4456989 ]\n",
      " [0.57149973]\n",
      " [0.47208689]\n",
      " [0.41562762]\n",
      " [0.4312716 ]\n",
      " [0.42595294]\n",
      " [0.38086717]\n",
      " [0.32265632]\n",
      " [0.32061443]\n",
      " [0.30310628]\n",
      " [0.24215361]\n",
      " [0.21470581]\n",
      " [0.19218729]\n",
      " [0.14228084]\n",
      " [0.14055857]\n",
      " [0.14793843]\n",
      " [0.16060056]\n",
      " [0.25357952]\n",
      " [0.23255237]\n",
      " [0.19757952]\n",
      " [0.17335919]\n",
      " [0.20299865]\n",
      " [0.18262949]\n",
      " [0.16369279]\n",
      " [0.17572692]\n",
      " [0.20413757]\n",
      " [0.20839492]\n",
      " [0.19897432]\n",
      " [0.17705837]\n",
      " [0.16234372]\n",
      " [0.17220791]\n",
      " [0.15763342]\n",
      " [0.14424497]\n",
      " [0.15429505]\n",
      " [0.15269798]\n",
      " [0.14656649]\n",
      " [0.16049833]\n",
      " [0.17233342]\n",
      " [0.17836738]\n",
      " [0.19168348]\n",
      " [0.21886734]\n",
      " [0.21635721]\n",
      " [0.22673563]\n",
      " [0.24246458]\n",
      " [0.25627537]\n",
      " [0.26097511]\n",
      " [0.28093095]\n",
      " [0.25306439]\n",
      " [0.26646324]\n",
      " [0.2683941 ]\n",
      " [0.25333333]\n",
      " [0.25428741]\n",
      " [0.26578743]\n",
      " [0.25063615]\n",
      " [0.26633621]\n",
      " [0.28796701]\n",
      " [0.25985001]\n",
      " [0.24712915]\n",
      " [0.25956038]\n",
      " [0.15955412]\n",
      " [0.12140678]\n",
      " [0.12880683]\n",
      " [0.11418671]\n",
      " [0.11686924]\n",
      " [0.11873804]\n",
      " [0.12013393]\n",
      " [0.12915438]\n",
      " [0.11690047]\n",
      " [0.11159884]\n",
      " [0.10808379]\n",
      " [0.0948332 ]\n",
      " [0.08562296]\n",
      " [0.10545156]\n",
      " [0.12351803]\n",
      " [0.12328937]\n",
      " [0.1148901 ]\n",
      " [0.10918268]\n",
      " [0.10483822]\n",
      " [0.10637156]\n",
      " [0.09966641]\n",
      " [0.09496354]\n",
      " [0.09743988]\n",
      " [0.10278355]\n",
      " [0.10314197]\n",
      " [0.10764934]\n",
      " [0.10960434]\n",
      " [0.1051668 ]\n",
      " [0.10278355]\n",
      " [0.10165857]\n",
      " [0.09942384]\n",
      " [0.09525317]\n",
      " [0.10109317]\n",
      " [0.09896043]\n",
      " [0.09426843]\n",
      " [0.09542281]\n",
      " [0.09320403]\n",
      " [0.08607189]\n",
      " [0.07880217]\n",
      " [0.0796404 ]\n",
      " [0.08899106]\n",
      " [0.09487154]\n",
      " [0.10688906]\n",
      " [0.12213085]\n",
      " [0.12207292]\n",
      " [0.12010344]\n",
      " [0.12101689]\n",
      " [0.11616447]\n",
      " [0.09956866]\n",
      " [0.09147461]\n",
      " [0.10310044]\n",
      " [0.10792373]\n",
      " [0.11237997]\n",
      " [0.14600465]\n",
      " [0.16391001]\n",
      " [0.16789242]\n",
      " [0.1889113 ]\n",
      " [0.20501336]\n",
      " [0.18980778]\n",
      " [0.20077862]\n",
      " [0.19632729]\n",
      " [0.21355177]\n",
      " [0.23293509]\n",
      " [0.39103801]\n",
      " [0.50030891]\n",
      " [0.52      ]\n",
      " [0.58956351]\n",
      " [0.88037548]\n",
      " [1.        ]\n",
      " [0.86257809]\n",
      " [0.72982329]\n",
      " [0.51888285]\n",
      " [0.34904295]\n",
      " [0.50115457]\n",
      " [0.48102405]\n",
      " [0.39581691]\n",
      " [0.55761619]\n",
      " [0.61784611]\n",
      " [0.51168348]\n",
      " [0.41958107]\n",
      " [0.40763382]\n",
      " [0.47923041]\n",
      " [0.47896974]\n",
      " [0.54797941]\n",
      " [0.53215671]\n",
      " [0.44304112]\n",
      " [0.4534678 ]\n",
      " [0.48318386]\n",
      " [0.45032325]\n",
      " [0.43718754]]\n",
      "Count of Training Data: 340\n",
      "\n",
      "Count of Test Data: 5\n",
      "\n",
      "Prediction (scaled): [[0.15996074]\n",
      " [0.15863174]\n",
      " [0.16537322]\n",
      " [0.15424513]\n",
      " [0.1598586 ]]\n",
      "Prediction (unscaled): 0.15996074175461164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with random values\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            hidden_input = np.dot(X, self.weights_input_hidden)\n",
    "            hidden_output = sigmoid(hidden_input)\n",
    "            output_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "            output = sigmoid(output_input)\n",
    "\n",
    "            # Backpropagation\n",
    "            error_output = y - output\n",
    "            d_output = error_output * sigmoid_derivative(output)\n",
    "            error_hidden = d_output.dot(self.weights_hidden_output.T)\n",
    "            d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
    "\n",
    "            # Update weights\n",
    "            self.weights_hidden_output += hidden_output.T.dot(d_output) * learning_rate\n",
    "            self.weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_input = np.dot(X, self.weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "        output = sigmoid(output_input)\n",
    "        return output\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "data = pd.read_csv('./BOGA.N0000.csv')\n",
    "\n",
    "# Assuming the dataset has columns 'Close (Rs.)' and 'Close (Rs.) old values'\n",
    "X = data[['Election']].values\n",
    "y = data[['Close (Rs.)']].values\n",
    "\n",
    "# Normalize the data (optional, but recommended)\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "y = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "# Print the training data\n",
    "print(\"Training Data:\")\n",
    "print(\"X (scaled):\", X)\n",
    "print(\"y (scaled):\", y)\n",
    "\n",
    "# Initialize and train the neural network\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 8  # It can adjust this as needed\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "nn.train(X, y, learning_rate, epochs)\n",
    "\n",
    "# Set Training Data Count to 340\n",
    "training_data_count = 340\n",
    "\n",
    "# Generate training data with the specified count\n",
    "training_data = np.random.rand(training_data_count, input_size)\n",
    "\n",
    "# Count of Training Data\n",
    "num_training_data = training_data.shape[0]\n",
    "print(\"Count of Training Data:\", num_training_data)\n",
    "\n",
    "# Set Test Data Count to 5\n",
    "test_data_count = 5\n",
    "\n",
    "# Generate test data with the specified count\n",
    "test_data = np.random.rand(test_data_count, input_size)\n",
    "\n",
    "# Count of Test Data\n",
    "num_test_data = test_data.shape[0]\n",
    "print(\"\\nCount of Test Data:\", num_test_data)\n",
    "\n",
    "prediction = nn.predict(test_data)\n",
    "print(\"\\nPrediction (scaled):\", prediction)\n",
    "\n",
    "# Inverse scaling to get the actual prediction\n",
    "scaled_prediction = prediction[0][0]\n",
    "unscaled_prediction = scaled_prediction * (y.max() - y.min()) + y.min()\n",
    "print(\"Prediction (unscaled):\", unscaled_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (scaled): [[0.27623509]]\n",
      "Prediction (unscaled): 0.2762350867958252\n",
      "Mean Squared Error (MSE): 0.0005647710996019612\n",
      "Root Mean Squared Error (RMSE): 0.023764913204174787\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with random values\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            hidden_input = np.dot(X, self.weights_input_hidden)\n",
    "            hidden_output = sigmoid(hidden_input)\n",
    "            output_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "            output = sigmoid(output_input)\n",
    "\n",
    "            # Backpropagation\n",
    "            error_output = y - output\n",
    "            d_output = error_output * sigmoid_derivative(output)\n",
    "            error_hidden = d_output.dot(self.weights_hidden_output.T)\n",
    "            d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
    "\n",
    "            # Update weights\n",
    "            self.weights_hidden_output += hidden_output.T.dot(d_output) * learning_rate\n",
    "            self.weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_input = np.dot(X, self.weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "        output = sigmoid(output_input)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Specify the label you want to select\n",
    "selected_label = 'ACL.N0000'\n",
    "\n",
    "# Replace 'your_data.csv' with your dataset file path\n",
    "data = pd.read_csv('./all_data.csv')\n",
    "data = data.loc[data['Label'] == selected_label]\n",
    "# Assuming your dataset has columns 'Close (Rs.)' and 'Close (Rs.) old values'\n",
    "X = data[['Close (Rs.)']].values\n",
    "y = data[['Close (Rs.)']].values\n",
    "\n",
    "# Normalize the data (optional, but recommended)\n",
    "X = (X - X.min()) / (X.max() - X.min())\n",
    "y = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "# Initialize and train the neural network\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 8  # You can adjust this as needed\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "nn.train(X, y, learning_rate, epochs)\n",
    "\n",
    "# Make predictions\n",
    "test_data = np.array([[0.3]])  # Replace with your test data\n",
    "prediction = nn.predict(test_data)\n",
    "print(\"Prediction (scaled):\", prediction)\n",
    "\n",
    "# Inverse scaling to get the actual prediction\n",
    "scaled_prediction = prediction[0][0]\n",
    "unscaled_prediction = scaled_prediction * (y.max() - y.min()) + y.min()\n",
    "print(\"Prediction (unscaled):\", unscaled_prediction)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
    "def calculate_mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    return np.sqrt(calculate_mse(y_true, y_pred))\n",
    "\n",
    "# Test data for calculating MSE and RMSE\n",
    "test_data = np.array([[0.3]])  # Replace with your test data\n",
    "y_true = test_data  # True values in the unscaled form\n",
    "y_pred = nn.predict(test_data) * (y.max() - y.min()) + y.min()\n",
    "\n",
    "mse = calculate_mse(y_true, y_pred)\n",
    "rmse = calculate_rmse(y_true, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for label: ACL.N0000, Scaled Prediction: 0.30166502811836604, Unscaled Prediction: 0.30166502811836604, MSE: 2.7723186349495957e-06, RMSE: 0.001665028118366052\n",
      "Training for label: ASIR.N0000, Scaled Prediction: 0.301853485335887, Unscaled Prediction: 0.301853485335887, MSE: 3.4354078903482506e-06, RMSE: 0.0018534853358870285\n",
      "Training for label: BFL.N0000, Scaled Prediction: 0.29967189340558953, Unscaled Prediction: 0.29967189340558953, MSE: 1.0765393729563008e-07, RMSE: 0.00032810659441046\n",
      "Training for label: BOGA.N0000, Scaled Prediction: 0.3060026890465471, Unscaled Prediction: 0.3060026890465471, MSE: 3.603227578953659e-05, RMSE: 0.006002689046547105\n",
      "Training for label: BRWN.N0000, Scaled Prediction: 0.28946354107565414, Unscaled Prediction: 0.28946354107565414, MSE: 0.00011101696666442728, RMSE: 0.010536458924345848\n",
      "Training for label: CARG.N0000, Scaled Prediction: 0.3120668468063991, Unscaled Prediction: 0.3120668468063991, MSE: 0.00014560879184910498, RMSE: 0.012066846806399134\n",
      "Training for label: CCS.N0000, Scaled Prediction: 0.25375881003835593, Unscaled Prediction: 0.25375881003835593, MSE: 0.002138247649068851, RMSE: 0.04624118996164406\n",
      "Training for label: CFIN.N0000, Scaled Prediction: 0.31727094939339096, Unscaled Prediction: 0.31727094939339096, MSE: 0.0002982856929490718, RMSE: 0.017270949393390966\n",
      "Training for label: CFLB.N0000, Scaled Prediction: 0.3107828616017303, Unscaled Prediction: 0.3107828616017303, MSE: 0.00011627010432206993, RMSE: 0.01078286160173031\n",
      "Training for label: CFVF.N0000, Scaled Prediction: 0.29883660684621455, Unscaled Prediction: 0.29883660684621455, MSE: 1.3534836302748357e-06, RMSE: 0.0011633931537854414\n",
      "Training for label: CIC.N0000, Scaled Prediction: 0.28269959321296745, Unscaled Prediction: 0.28269959321296745, MSE: 0.0002993040749968016, RMSE: 0.01730040678703254\n",
      "Training for label: CIC.X0000, Scaled Prediction: 0.26507606322167987, Unscaled Prediction: 0.26507606322167987, MSE: 0.0012196813600961007, RMSE: 0.03492393677832012\n",
      "Training for label: CIND.N0000, Scaled Prediction: 0.30142219895787686, Unscaled Prediction: 0.30142219895787686, MSE: 2.022649875786068e-06, RMSE: 0.0014221989578768746\n",
      "Training for label: CINV.N0000, Scaled Prediction: 0.3007661937898464, Unscaled Prediction: 0.3007661937898464, MSE: 5.870529235992455e-07, RMSE: 0.0007661937898464366\n",
      "Training for label: COMB.N0000, Scaled Prediction: 0.2958811484678579, Unscaled Prediction: 0.2958811484678579, MSE: 1.6964937943829218e-05, RMSE: 0.004118851532142087\n",
      "Training for label: CTC.N0000, Scaled Prediction: 0.26987338050516485, Unscaled Prediction: 0.26987338050516485, MSE: 0.0009076132021865806, RMSE: 0.03012661949483514\n",
      "Training for label: DFCC.N0000, Scaled Prediction: 0.28007808396556605, Unscaled Prediction: 0.28007808396556605, MSE: 0.0003968827384830361, RMSE: 0.01992191603443394\n",
      "Training for label: DIPD.N0000, Scaled Prediction: 0.3280379429816202, Unscaled Prediction: 0.3280379429816202, MSE: 0.000786126246640586, RMSE: 0.028037942981620212\n",
      "Training for label: DIST.N0000, Scaled Prediction: 0.2933519760052619, Unscaled Prediction: 0.2933519760052619, MSE: 4.419622303461316e-05, RMSE: 0.006648023994738073\n",
      "Training for label: EAST.N0000, Scaled Prediction: 0.3028046704241753, Unscaled Prediction: 0.3028046704241753, MSE: 7.866176188243742e-06, RMSE: 0.002804670424175315\n",
      "Training for label: GHLL.N0000, Scaled Prediction: 0.301326867422365, Unscaled Prediction: 0.301326867422365, MSE: 1.7605771565335012e-06, RMSE: 0.0013268674223649857\n",
      "Training for label: GLAS.N0000, Scaled Prediction: 0.3059374933985867, Unscaled Prediction: 0.3059374933985867, MSE: 3.525382785826106e-05, RMSE: 0.005937493398586735\n",
      "Training for label: GRAN.N0000, Scaled Prediction: 0.30447343141441074, Unscaled Prediction: 0.30447343141441074, MSE: 2.0011588619437014e-05, RMSE: 0.004473431414410756\n",
      "Training for label: HAYC.N0000, Scaled Prediction: 0.34518445886481214, Unscaled Prediction: 0.34518445886481214, MSE: 0.002041635322905901, RMSE: 0.04518445886481215\n",
      "Training for label: HAYL.N0000, Scaled Prediction: 0.30035328897358504, Unscaled Prediction: 0.30035328897358504, MSE: 1.2481309885678032e-07, RMSE: 0.00035328897358505307\n",
      "Training for label: HNB.N0000, Scaled Prediction: 0.27334854059104385, Unscaled Prediction: 0.27334854059104385, MSE: 0.0007103002886272366, RMSE: 0.026651459408956135\n",
      "Training for label: JKH.N0000, Scaled Prediction: 0.2961831927647328, Unscaled Prediction: 0.2961831927647328, MSE: 1.4568017471187835e-05, RMSE: 0.0038168072352671722\n",
      "Training for label: LMF.N0000, Scaled Prediction: 0.3082520223913211, Unscaled Prediction: 0.3082520223913211, MSE: 6.8095873546865e-05, RMSE: 0.008252022391321112\n",
      "Training for label: LWL.N0000, Scaled Prediction: 0.30210216401208545, Unscaled Prediction: 0.30210216401208545, MSE: 4.419093533707243e-06, RMSE: 0.0021021640120854612\n",
      "Training for label: NDB.N0000, Scaled Prediction: 0.3030210377494363, Unscaled Prediction: 0.3030210377494363, MSE: 9.126669083519114e-06, RMSE: 0.003021037749436295\n",
      "Training for label: NEST.N0000, Scaled Prediction: 0.2762373968071072, Unscaled Prediction: 0.2762373968071072, MSE: 0.0005646613105028794, RMSE: 0.023762603192892806\n",
      "Training for label: ONAL.N0000, Scaled Prediction: 0.2684177338728783, Unscaled Prediction: 0.2684177338728783, MSE: 0.000997439533724337, RMSE: 0.03158226612712167\n",
      "Training for label: OSEA.N0000, Scaled Prediction: 0.2900937270707227, Unscaled Prediction: 0.2900937270707227, MSE: 9.813424334933257e-05, RMSE: 0.009906272929277316\n",
      "Training for label: PALM.N0000, Scaled Prediction: 0.2944301014328868, Unscaled Prediction: 0.2944301014328868, MSE: 3.102377004792976e-05, RMSE: 0.005569898567113207\n",
      "Training for label: PARQ.N0000, Scaled Prediction: 0.3037928854785868, Unscaled Prediction: 0.3037928854785868, MSE: 1.4385980253674656e-05, RMSE: 0.003792885478586805\n",
      "Training for label: PDL.N0000, Scaled Prediction: 0.2950470977770401, Unscaled Prediction: 0.2950470977770401, MSE: 2.4531240430201213e-05, RMSE: 0.0049529022229599096\n",
      "Training for label: RCL.N0000, Scaled Prediction: 0.29570675080161085, Unscaled Prediction: 0.29570675080161085, MSE: 1.8431988679468972e-05, RMSE: 0.004293249198389137\n",
      "Training for label: RICH.N0000, Scaled Prediction: 0.27266959896747, Unscaled Prediction: 0.27266959896747, MSE: 0.000746950820598916, RMSE: 0.027330401032529983\n",
      "Training for label: SAMP.N0000, Scaled Prediction: 0.2983825470816429, Unscaled Prediction: 0.2983825470816429, MSE: 2.616153943101823e-06, RMSE: 0.0016174529183570763\n",
      "Training for label: SEMB.N0000, Scaled Prediction: 0.31062251362686544, Unscaled Prediction: 0.31062251362686544, MSE: 0.00011283779575294214, RMSE: 0.010622513626865449\n",
      "Training for label: SEYB.N0000, Scaled Prediction: 0.28768039955716235, Unscaled Prediction: 0.28768039955716235, MSE: 0.0001517725550711654, RMSE: 0.012319600442837642\n",
      "Training for label: SHOT.N0000, Scaled Prediction: 0.3149455279526248, Unscaled Prediction: 0.3149455279526248, MSE: 0.00022336880578269002, RMSE: 0.014945527952624826\n",
      "Training for label: SPEN.N0000, Scaled Prediction: 0.30989974042811463, Unscaled Prediction: 0.30989974042811463, MSE: 9.800486054404754e-05, RMSE: 0.009899740428114645\n",
      "Training for label: TILE.N0000, Scaled Prediction: 0.3082946885281865, Unscaled Prediction: 0.3082946885281865, MSE: 6.880185777962854e-05, RMSE: 0.008294688528186489\n",
      "Training for label: TKYO.N0000, Scaled Prediction: 0.29933336759791224, Unscaled Prediction: 0.29933336759791224, MSE: 4.443987595132867e-07, RMSE: 0.0006666324020877523\n",
      "Training for label: TYRE.N0000, Scaled Prediction: 0.29604836491545233, Unscaled Prediction: 0.29604836491545233, MSE: 1.5615419841427956e-05, RMSE: 0.0039516350845476556\n",
      "Mean Values:\n",
      "Prediction (scaled)      0.297242\n",
      "Prediction (unscaled)    0.297242\n",
      "MSE                      0.000274\n",
      "RMSE                     0.011492\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Define the neural network class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with random values\n",
    "        self.weights_input_hidden = np.random.rand(input_size, hidden_size)\n",
    "        self.weights_hidden_output = np.random.rand(hidden_size, output_size)\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            hidden_input = np.dot(X, self.weights_input_hidden)\n",
    "            hidden_output = sigmoid(hidden_input)\n",
    "            output_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "            output = sigmoid(output_input)\n",
    "\n",
    "            # Backpropagation\n",
    "            error_output = y - output\n",
    "            d_output = error_output * sigmoid_derivative(output)\n",
    "            error_hidden = d_output.dot(self.weights_hidden_output.T)\n",
    "            d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
    "\n",
    "            # Update weights\n",
    "            self.weights_hidden_output += hidden_output.T.dot(d_output) * learning_rate\n",
    "            self.weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n",
    "\n",
    "    def predict(self, X):\n",
    "        hidden_input = np.dot(X, self.weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output_input = np.dot(hidden_output, self.weights_hidden_output)\n",
    "        output = sigmoid(output_input)\n",
    "        return output\n",
    "\n",
    "# Function to train the neural network using a DataFrame and return output\n",
    "def train_neural_network(data):\n",
    "    # Assuming the dataset has columns 'Close (Rs.)' and 'Close (Rs.) old values'\n",
    "    X = data[['Close (Rs.)']].values\n",
    "    y = data[['Close (Rs.)']].values\n",
    "\n",
    "    # Normalize the data (optional, but recommended)\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    y = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "    # Initialize and train the neural network\n",
    "    input_size = X.shape[1]\n",
    "    hidden_size = 8  # You can adjust this as needed\n",
    "    output_size = 1\n",
    "    learning_rate = 0.1\n",
    "    epochs = 10000\n",
    "\n",
    "    nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "    nn.train(X, y, learning_rate, epochs)\n",
    "\n",
    "    # Make predictions\n",
    "    test_data = np.array([[0.3]])  # Replace with the test data\n",
    "    prediction = nn.predict(test_data)\n",
    "    scaled_prediction = prediction[0][0]\n",
    "\n",
    "    # Inverse scaling to get the actual prediction\n",
    "    unscaled_prediction = scaled_prediction * (y.max() - y.min()) + y.min()\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
    "    def calculate_mse(y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def calculate_rmse(y_true, y_pred):\n",
    "        return np.sqrt(calculate_mse(y_true, y_pred))\n",
    "\n",
    "    # Test data for calculating MSE and RMSE\n",
    "    test_data = np.array([[0.3]])  # Replace with the test data\n",
    "    y_true = test_data  # True values in the unscaled form\n",
    "    y_pred = nn.predict(test_data) * (y.max() - y.min()) + y.min()\n",
    "\n",
    "    mse = calculate_mse(y_true, y_pred)\n",
    "    rmse = calculate_rmse(y_true, y_pred)\n",
    "\n",
    "    return scaled_prediction, unscaled_prediction, mse, rmse\n",
    "\n",
    "# Load the complete dataset from a CSV file\n",
    "data = pd.read_csv('./all_data.csv')\n",
    "\n",
    "# Group the data by the 'Label' column and iterate through groups\n",
    "results = []\n",
    "\n",
    "for label, group in data.groupby('Label'):\n",
    "    scaled_prediction, unscaled_prediction, mse, rmse = train_neural_network(group)\n",
    "    message = f\"Training for label: {label}, Scaled Prediction: {scaled_prediction}, Unscaled Prediction: {unscaled_prediction}, MSE: {mse}, RMSE: {rmse}\"\n",
    "    print(message)\n",
    "    results.append((scaled_prediction, unscaled_prediction, mse, rmse))\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "result_df = pd.DataFrame(results, columns=['Prediction (scaled)', 'Prediction (unscaled)', 'MSE', 'RMSE'])\n",
    "\n",
    "# Calculate the mean for each column\n",
    "mean_values = result_df.mean()\n",
    "print(\"Mean Values:\")\n",
    "print(mean_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
